---
---

@string{aps = {American Physical Society,}}

@article{
lin2023mixture,
abbr={TMLR},
bibtex_show={true},
title={Mixture of Dynamical Variational Autoencoders for Multi-Source Trajectory Modeling and Separation},
author={Xiaoyu Lin and Laurent Girin and Xavier Alameda-Pineda},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2023},
pdf={https://arxiv.org/abs/2312.04167},
url={https://openreview.net/forum?id=sbkZKBVC31},
code={https://github.com/linxiaoyu1/MixDVAE},
poster={MixDVAE_poster.pdf},
selected={true},
abstract={In this paper, we propose a latent-variable generative model called mixture of dynamical variational autoencoders (MixDVAE) to model the dynamics of a system composed of multiple moving sources. A DVAE model is pre-trained on a single-source dataset to capture the source dynamics. Then, multiple instances of the pre-trained DVAE model are integrated into a multi-source mixture model with a discrete observation-to-source assignment latent variable. The posterior distributions of both the discrete observation-to-source assignment variable and the continuous DVAE variables representing the sources content/position are estimated using the variational expectation-maximization algorithm, leading to multi-source trajectories estimation.  We illustrate the versatility of the proposed MixDVAE model on two tasks: a computer vision task, namely multi-object tracking, and  an audio processing task, namely single-channel audio source separation. Experimental results show that the proposed method works well on these two tasks, and outperforms several baseline methods.}
}

@INPROCEEDINGS{10096751,
  author={Lin, Xiaoyu and Bie, Xiaoyu and Leglaive, Simon and Girin, Laurent and Alameda-Pineda, Xavier},
  abbr={ICASSP},
  bibtex_show={true},
  booktitle={IEEE International Conference on Acoustics, Speech and Signal Processing}, 
  title={Speech Modeling with a Hierarchical Transformer Dynamical VAE}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  doi={10.1109/ICASSP49357.2023.10096751},
  location = {Rhodes, Greece},
  pdf={https://arxiv.org/abs/2303.09404},
  code={https://gitlab.inria.fr/robotlearn/light-dvae},
  poster={ICASSP2023Poster.pdf},
  abstract={The dynamical variational autoencoders (DVAEs) are a family of latent-variable deep generative models that extends the VAE to model a sequence of observed data and a corresponding sequence of latent vectors. In almost all the DVAEs of the literature, the temporal dependencies within each sequence and across the two sequences are modeled with recurrent neural networks. In this paper, we propose to model speech signals with the Hierarchical Transformer DVAE (HiT-DVAE), which is a DVAE with two levels of latent variable (sequence-wise and frame-wise) and in which the temporal dependencies are implemented with the Transformer architecture. We show that HiT-DVAE outperforms several other DVAEs for speech spectrogram modeling, while enabling a simpler training procedure, revealing its high potential for downstream low-level speech processing tasks such as speech enhancement.
}
}

@INPROCEEDINGS{interspeech2023,
  author={Lin, Xiaoyu and Leglaive, Simon and Girin, Laurent and Alameda-Pineda, Xavier},
  abbr={INTERSPEECH},
  bibtex_show={true},
  booktitle={Proceedings Interspeech}, 
  title={Unsupervised speech enhancement with deep dynamical generative speech and noise models}, 
  year={2023},
  volume={},
  number={},
  location = {Dublin, Ireland},
  pdf={https://arxiv.org/abs/2306.07820},
  code={https://gitlab.inria.fr/xilin/ddgm_se},
  slides={presentation_DDGM_SE.pdf},
  abstract={This work builds on a previous work on unsupervised speech enhancement using a dynamical variational autoencoder (DVAE) as the clean speech model and non-negative matrix factorization (NMF) as the noise model. We propose to replace the NMF noise model with a deep dynamical generative model (DDGM) depending either on the DVAE latent variables, or on the noisy observations, or on both. This DDGM can be trained in three configurations: noise-agnostic, noise-dependent and noise adaptation after noise-dependent training. Experimental results show that the proposed method achieves competitive performance compared to state-of-the-art unsupervised speech enhancement methods, while the noise-dependent training configuration yields a much more time-efficient inference process.}
}
